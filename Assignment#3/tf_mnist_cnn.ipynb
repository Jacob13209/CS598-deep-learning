{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment *3* - CS598 - Deep Learning \n",
    "\n",
    "__MNIST with Tensorflow/CNN__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries \n",
    "\n",
    "For this homework, it is using tensorflow for the MNIST problem with CNN approach. Based on the example in the textbook, libraries being used are mainly from tensorflow, including mnist dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import warnings library to ignore any warnings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check the version of tensorflow to verify it will work with the examople from the textbook (1.14.0)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-5c81c5f4c9da>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/homework2/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/homework2/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/homework2/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/homework2/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/homework2/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSz = 100\n",
    "learningRate = 0.5\n",
    "epoches = 10 \n",
    "\n",
    "# calculate number of batches for training/test data\n",
    "trainingBatches = int(len(mnist.train.labels) / batchSz) * epoches \n",
    "testBatches = int(len(mnist.test.labels) / batchSz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, [batchSz, 784])\n",
    "ans = tf.placeholder(tf.float32, [batchSz, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple TF Convolution Example from textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "W = tf.Variable(tf.random_normal([784, 10],stddev=.1))\n",
    "b = tf.Variable(tf.random_normal([10],stddev=.1))\n",
    "\n",
    "#reshape the input images\n",
    "image = tf.reshape(img, [batchSz, 28, 28, 1]) \n",
    "# initialize filter parameters, size, channel, number of features \n",
    "flts = tf.Variable(tf.truncated_normal([5, 5, 1, 4], stddev = 0.1))\n",
    "# convolution layer parameters, input, strike, channel \n",
    "convOut = tf.nn.conv2d(image, flts, [1, 2, 2, 1], \"SAME\")\n",
    "# activation \n",
    "convOut = tf.nn.relu(convOut)\n",
    "# flatten the output 784 = 14(28/2) * 14(28/2) * 4\n",
    "convOut = tf.reshape(convOut, [100, 784])\n",
    "# apply the linear function\n",
    "prbs = tf.nn.softmax(tf.matmul(convOut, W) + b)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multilevel Convolution implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the input images\n",
    "image = tf.reshape(img, [batchSz, 28, 28, 1])\n",
    "\n",
    "##### First level #####\n",
    "# filter initialization, size, channel, number of features\n",
    "flts = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "# convolution layer parameters, input, strike, channel \n",
    "convOut = tf.nn.conv2d(image, flts, [1, 2, 2, 1], \"SAME\")\n",
    "# activation \n",
    "convOut = tf.nn.relu(convOut)\n",
    "\n",
    "##### Second level #####\n",
    "# filter initialization, size, channel(32 is the features from first level)\n",
    "# number of features \n",
    "flts2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "# convolution layer parameters, input, strike, channel \n",
    "convOut2 = tf.nn.conv2d(convOut, flts2, [1,  2,  2,  1], \"SAME\")\n",
    "\n",
    "# flatten the output 3136 = 7(28/2/2) * 7(28/2/2) * 64(features from flts2) \n",
    "convOut2 = tf.reshape(convOut2, [batchSz, 3136])\n",
    "\n",
    "# weights/biases \n",
    "W = tf.Variable(tf.random_normal([3136,10],stddev=0.1))\n",
    "b = tf.Variable(tf.random_normal([10], stddev=0.1))\n",
    "\n",
    "# apply the linear function to get probability distributions \n",
    "prbs = tf.nn.softmax(tf.matmul(convOut2, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xEnt = tf.reduce_mean(-tf.reduce_sum(ans * tf.log(prbs), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(learningRate).minimize(xEnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCorrect = tf.equal(tf.argmax(prbs,1), tf.argmax(ans,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(numCorrect, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Train Accuracy: 0.08\n",
      "Batch: 100, Train Accuracy: 0.93\n",
      "Batch: 200, Train Accuracy: 0.95\n",
      "Batch: 300, Train Accuracy: 0.99\n",
      "Batch: 400, Train Accuracy: 0.93\n",
      "Batch: 500, Train Accuracy: 0.95\n",
      "Batch: 600, Train Accuracy: 0.99\n",
      "Batch: 700, Train Accuracy: 0.97\n",
      "Batch: 800, Train Accuracy: 0.99\n",
      "Batch: 900, Train Accuracy: 0.95\n",
      "Batch: 1000, Train Accuracy: 1.0\n",
      "Batch: 1100, Train Accuracy: 0.96\n",
      "Batch: 1200, Train Accuracy: 0.98\n",
      "Batch: 1300, Train Accuracy: 0.99\n",
      "Batch: 1400, Train Accuracy: 0.96\n",
      "Batch: 1500, Train Accuracy: 0.95\n",
      "Batch: 1600, Train Accuracy: 0.96\n",
      "Batch: 1700, Train Accuracy: 0.95\n",
      "Batch: 1800, Train Accuracy: 0.99\n",
      "Batch: 1900, Train Accuracy: 0.99\n",
      "Batch: 2000, Train Accuracy: 0.99\n",
      "Batch: 2100, Train Accuracy: 0.99\n",
      "Batch: 2200, Train Accuracy: 0.99\n",
      "Batch: 2300, Train Accuracy: 0.99\n",
      "Batch: 2400, Train Accuracy: 0.98\n",
      "Batch: 2500, Train Accuracy: 0.97\n",
      "Batch: 2600, Train Accuracy: 0.98\n",
      "Batch: 2700, Train Accuracy: 0.99\n",
      "Batch: 2800, Train Accuracy: 1.0\n",
      "Batch: 2900, Train Accuracy: 0.98\n",
      "Batch: 3000, Train Accuracy: 0.99\n",
      "Batch: 3100, Train Accuracy: 0.99\n",
      "Batch: 3200, Train Accuracy: 0.97\n",
      "Batch: 3300, Train Accuracy: 1.0\n",
      "Batch: 3400, Train Accuracy: 0.98\n",
      "Batch: 3500, Train Accuracy: 0.96\n",
      "Batch: 3600, Train Accuracy: 0.99\n",
      "Batch: 3700, Train Accuracy: 0.96\n",
      "Batch: 3800, Train Accuracy: 1.0\n",
      "Batch: 3900, Train Accuracy: 0.99\n",
      "Batch: 4000, Train Accuracy: 0.99\n",
      "Batch: 4100, Train Accuracy: 0.98\n",
      "Batch: 4200, Train Accuracy: 0.97\n",
      "Batch: 4300, Train Accuracy: 0.97\n",
      "Batch: 4400, Train Accuracy: 0.98\n",
      "Batch: 4500, Train Accuracy: 1.0\n",
      "Batch: 4600, Train Accuracy: 0.97\n",
      "Batch: 4700, Train Accuracy: 0.97\n",
      "Batch: 4800, Train Accuracy: 1.0\n",
      "Batch: 4900, Train Accuracy: 0.98\n",
      "Batch: 5000, Train Accuracy: 0.99\n",
      "Batch: 5100, Train Accuracy: 0.97\n",
      "Batch: 5200, Train Accuracy: 0.99\n",
      "Batch: 5300, Train Accuracy: 0.98\n",
      "Batch: 5400, Train Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "for i in range(trainingBatches):\n",
    "    imgs, anss = mnist.train.next_batch(batchSz)\n",
    "    # textbook p.38, python's combining computation\n",
    "    train_acc,ignore = sess.run([accuracy,train], feed_dict = {img: imgs, ans: anss})\n",
    "    if i % 100 == 0:\n",
    "        print(\"Batch: %r, Train Accuracy: %r\" % (i, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9769000095129013\n"
     ]
    }
   ],
   "source": [
    "sumAcc=0\n",
    "for i in range(testBatches):\n",
    "    imgs, anss = mnist.test.next_batch(batchSz)\n",
    "    sumAcc+=sess.run(accuracy, feed_dict={img: imgs, ans: anss})\n",
    "print(\"Test Accuracy: %r\" % (sumAcc/testBatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been saved to path: Model/tf_mnist_cnn.ckpt\n"
     ]
    }
   ],
   "source": [
    "# save the trained model \n",
    "model_saver = tf.train.Saver()\n",
    "save_path = model_saver.save(sess, 'Model/tf_mnist_cnn.ckpt')\n",
    "print('Model has been saved to path:', save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*Introduction to Deep Learning by Eugene Charniak*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
