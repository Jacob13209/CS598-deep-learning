{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Assignment *1* - CS 598 - Deep Learning\n",
    "\n",
    "####  *Backpropagation Neural Network* ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries ## \n",
    "\n",
    "The libraries being used for are mainly numpy, Keras are also be used only for loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with Keras ##\n",
    "\n",
    "The following code block uses Keras to load the mnist dataset and apply normalization by reshaping the matrix and normalizing values to between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_mnist():\n",
    "    '''\n",
    "    function to load the dataset and perform normalization \n",
    "    returns the train/test dataset \n",
    "    '''\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # reshape the matrix \n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]).astype('float32')\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2]).astype('float32')\n",
    "    \n",
    "    # maximum value of a pixel is 255 \n",
    "    # normalize the value to range 0 - 1\n",
    "    x_train = x_train/255\n",
    "    x_test = x_test/255\n",
    "    \n",
    "    return (x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset by calling the defined function above \n",
    "# and print out the shape for confirmation \n",
    "x_train, x_test, y_train, y_test = load_dataset_mnist()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class ## \n",
    "\n",
    "Self-defined class for a neural network that is based on the combination of feed forward and backpropagtion. \n",
    "\n",
    "During the initialization of the class object, it will randomly generate weights and biases for each layer and set the value close 0, it uses sigmoid as the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following sequence are being covered: \n",
    "Weight/Bias random initialization -> feed forward -> calculate the actual output -> calculate the loss function -> loss/error -> derivative of error -> gradient -> backpropagate -> updates weight/bias based on delta rule -> feed forward ... loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork():\n",
    "    '''\n",
    "    Self-defined class, a implementation of feed-forward, back-propagation neural network \n",
    "    It supports multiple layers and accepts \n",
    "    '''\n",
    "    def __init__(self, sizes, learning_rate, batch_size):\n",
    "        '''\n",
    "        default constructor for MyNeuralNetwork, accepts inputs like number of neurons for each layer, learning rate,\n",
    "        batch size. \n",
    "        '''\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size \n",
    "\n",
    "        # initilaize the weights/biases with random number, but close to zero\n",
    "        self.weights = [np.random.randn(i, j)*0.01 for i, j in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.bias = [np.random.randn(1, j)*0.01 for j in self.sizes[1:]]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "        Activation function that used for predicting probability based outputs\n",
    "        return value usually between -1 and 1\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        '''\n",
    "        Calculate the derivative of the sigmoid of x\n",
    "        '''\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1-s)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        '''\n",
    "        Softmax function is being used for prediction in multi-class models,\n",
    "        returns probabilities of each class in a group of different classes\n",
    "        z: output \n",
    "        '''      \n",
    "        # get the max \n",
    "        max_row = np.max(z, axis=-1, keepdims=True)  \n",
    "        tmp = z - max_row\n",
    "        return np.exp(tmp) / np.sum(np.exp(tmp), axis=-1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y, a):\n",
    "        '''\n",
    "        compute loss function\n",
    "        y: expected output\n",
    "        a: actual output\n",
    "        '''\n",
    "        m = y.shape[0]\n",
    "        s = a[range(m), y]\n",
    "        l_sum = np.sum(np.log(s))\n",
    "        l = -(1./m) * l_sum\n",
    "        return l\n",
    "\n",
    "    \n",
    "    def cost_derivative(self, y, a):\n",
    "        '''\n",
    "        compute loss function\n",
    "        y: expected output\n",
    "        a: actual output\n",
    "        '''\n",
    "        m = y.shape[0]\n",
    "        a[range(m), y] -= 1\n",
    "        return a\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        '''\n",
    "        Get the output based on current weight/bias \n",
    "        '''\n",
    "        a = x\n",
    "        for w, b in zip(self.weights[:-1], self.bias[:-1]):\n",
    "            #print(w.shape)\n",
    "            #print(b.shape)   \n",
    "            z = np.dot(a, w) + b\n",
    "            a = self.sigmoid(z) \n",
    "        l = np.dot(a, self.weights[-1]) + self.bias[-1]\n",
    "        return l\n",
    "\n",
    "    def backward_backpropagate(self, x, y):\n",
    "        '''\n",
    "        container to capture the steps involved for training a neural network\n",
    "        feed forward, actual output, loss, back propagate\n",
    "        x, y: training data \n",
    "        returns dws, dbs: lists of weights/biases for each layer \n",
    "        '''\n",
    "        # init the weights/biases to 0 for each layer  \n",
    "        # and the shape varies between layers \n",
    "        dbs = [np.zeros(b.shape) for b in self.bias]\n",
    "        dws = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # zs: list of intermediate outputs\n",
    "        # activations: list of activations \n",
    "        zs = [] \n",
    "        activations = []\n",
    "\n",
    "        a = x\n",
    "        activations.append(a)\n",
    "        # feed forward \n",
    "        for weight, bias in zip(self.weights[:-1], self.bias[:-1]):\n",
    "            z = np.dot(a, weight) + bias\n",
    "            zs.append(z)           \n",
    "            a = self.sigmoid(z)\n",
    "            activations.append(a)  \n",
    "        \n",
    "        # output layer \n",
    "        logits = np.dot(a, self.weights[-1]) + self.bias[-1]\n",
    "        zs.append(logits)\n",
    "        #a, loss = self.softmax_cross_entropy(logits, y)\n",
    "        a = self.softmax(logits)\n",
    "        loss = self.cross_entropy_loss(y, a)\n",
    "        activations.append(a)\n",
    "        \n",
    "        # back propagate \n",
    "        #dl = self.derivation_softmax_cross_entropy(logits, y) # calculate the delta between actual output and expected output\n",
    "        dl = self.cost_derivative(y, a)\n",
    "        # update weight/bias for the output layer \n",
    "        dws[-1] = np.dot(activations[-2].T, dl) \n",
    "        dbs[-1] = np.sum(dl, axis=0, keepdims=True)\n",
    "\n",
    "        # back propagate the delta if there are more than 2 layers (input and output)\n",
    "        for i in range(2, self.num_layers):\n",
    "            dl = np.dot(dl, self.weights[-i+1].T) * self.sigmoid_derivative(zs[-i])\n",
    "            dws[-i] = np.dot(activations[-i-1].T, dl)\n",
    "            dbs[-i] = np.sum(dl, axis=0, keepdims=True)\n",
    "        \n",
    "        return loss, dws, dbs\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test):\n",
    "        '''\n",
    "        Calculate the accurancy for test data\n",
    "        x_test, y_test \n",
    "        '''\n",
    "        cnt = 0\n",
    "        n = len(x_test)\n",
    "        \n",
    "        for x, y in zip(x_test, y_test):\n",
    "            output = self.feed_forward(x)\n",
    "            res = np.argmax(output) # get highest possibility \n",
    "            \n",
    "            correct = np.sum(res == y)\n",
    "            cnt += correct\n",
    "        \n",
    "        acc = cnt / n            \n",
    "        return acc  \n",
    "\n",
    "    \n",
    "    def adjust_neural_network(self, lr, sz, dws, dbs):    \n",
    "        '''\n",
    "        Update the weight/bias based on learning rate\n",
    "        Use avrage since we process by batch\n",
    "        lr: learning rate\n",
    "        sz: batch size\n",
    "        dws: [weights]\n",
    "        dbs: [biases]\n",
    "        '''            \n",
    "        self.weights = [w - (lr/sz) * dw for w, dw in zip(self.weights, dws)]\n",
    "        self.bias = [b - (lr/sz) * db for b, db in zip(self.bias, dbs)]\n",
    "\n",
    "    def train_neural_network(self, x_train, y_train, x_test, y_test, epoches):\n",
    "        '''\n",
    "        function to train the neural network by calling all the step functions \n",
    "        process data in batches and print the accurancy result \n",
    "        x_train, y_train: training data\n",
    "        x_test, y_test: test data \n",
    "        epoches: number of training \n",
    "        '''\n",
    "        val_accs = []\n",
    "        bs = self.batch_size\n",
    "        lr = self.learning_rate\n",
    "\n",
    "        sz = len(x_train)\n",
    "        tsz = len(x_test)\n",
    "\n",
    "        for epoch in range(epoches):        \n",
    "            \n",
    "            # get data into batches based on batch size \n",
    "            x_batches = [x_train[x:x + bs] for x in range(0,sz,bs)]\n",
    "            y_batches = [y_train[y:y + bs] for y in range(0,sz,bs)] \n",
    "            \n",
    "            for i, (x, y) in enumerate(zip(x_batches, y_batches)):\n",
    "                # backward propagation by batches \n",
    "                loss, dws, dbs = self.backward_backpropagate(x, y)\n",
    "                # adjust weights/biases \n",
    "                self.adjust_neural_network(lr, bs, dws, dbs)\n",
    "\n",
    "                #if i % 100 == 0:\n",
    "                #    print(\"Epoch {}, batch {}, loss {}\".format(epoch, i, loss))\n",
    "\n",
    "            \"\"\"\n",
    "            # spilt test data into batches \n",
    "            x_test_batches = [x_test[x:x + bs] for x in range(0,tsz,bs)]\n",
    "            y_test_batches = [y_test[y:y + bs] for y in range(0,tsz,bs)] \n",
    "\n",
    "            for j, (x, y) in enumerate(zip(x_test_batches, y_test_batches)):\n",
    "                acc = self.get_accuracy(x, y)\n",
    "                val_accs.append(acc)\n",
    "\n",
    "                if j % 100 == 0:\n",
    "                    print(\"Epoch {}, Batch {}, Average Accuracy {}\".format(epoch, j, np.average(val_accs)))\n",
    "                    val_accs=[]\n",
    "            \"\"\"\n",
    "            # calculate the accuracy for test data \n",
    "            acc = self.get_accuracy(x_test, y_test)\n",
    "            print(\"Epoch: {}, Average Accuracy = {}\".format(epoch, acc))\n",
    "            \n",
    "            # break the training if reaches 91.5% accuracy \n",
    "            if acc >= 0.915: \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution ##\n",
    "\n",
    "Initialize an self-defined Neural Network object with the following parameters:\n",
    "- A list of number of neurons for each layer\n",
    "- Batch Size\n",
    "- Learning Rate\n",
    "- Max Epoches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average Accuracy = 0.8324\n",
      "Epoch: 1, Average Accuracy = 0.8563\n",
      "Epoch: 2, Average Accuracy = 0.8678\n",
      "Epoch: 3, Average Accuracy = 0.875\n",
      "Epoch: 4, Average Accuracy = 0.8792\n",
      "Epoch: 5, Average Accuracy = 0.8828\n",
      "Epoch: 6, Average Accuracy = 0.8852\n",
      "Epoch: 7, Average Accuracy = 0.8884\n",
      "Epoch: 8, Average Accuracy = 0.8906\n",
      "Epoch: 9, Average Accuracy = 0.8924\n",
      "Epoch: 10, Average Accuracy = 0.8938\n",
      "Epoch: 11, Average Accuracy = 0.8956\n",
      "Epoch: 12, Average Accuracy = 0.897\n",
      "Epoch: 13, Average Accuracy = 0.8988\n",
      "Epoch: 14, Average Accuracy = 0.9002\n",
      "Epoch: 15, Average Accuracy = 0.901\n",
      "Epoch: 16, Average Accuracy = 0.9016\n",
      "Epoch: 17, Average Accuracy = 0.9027\n",
      "Epoch: 18, Average Accuracy = 0.9038\n",
      "Epoch: 19, Average Accuracy = 0.9048\n",
      "Epoch: 20, Average Accuracy = 0.9053\n",
      "Epoch: 21, Average Accuracy = 0.9063\n",
      "Epoch: 22, Average Accuracy = 0.9064\n",
      "Epoch: 23, Average Accuracy = 0.9065\n",
      "Epoch: 24, Average Accuracy = 0.907\n",
      "Epoch: 25, Average Accuracy = 0.9068\n",
      "Epoch: 26, Average Accuracy = 0.9075\n",
      "Epoch: 27, Average Accuracy = 0.908\n",
      "Epoch: 28, Average Accuracy = 0.9088\n",
      "Epoch: 29, Average Accuracy = 0.9093\n",
      "Epoch: 30, Average Accuracy = 0.9095\n",
      "Epoch: 31, Average Accuracy = 0.9101\n",
      "Epoch: 32, Average Accuracy = 0.9102\n",
      "Epoch: 33, Average Accuracy = 0.9103\n",
      "Epoch: 34, Average Accuracy = 0.9103\n",
      "Epoch: 35, Average Accuracy = 0.9107\n",
      "Epoch: 36, Average Accuracy = 0.9115\n",
      "Epoch: 37, Average Accuracy = 0.912\n",
      "Epoch: 38, Average Accuracy = 0.912\n",
      "Epoch: 39, Average Accuracy = 0.9122\n",
      "Epoch: 40, Average Accuracy = 0.9126\n",
      "Epoch: 41, Average Accuracy = 0.9131\n",
      "Epoch: 42, Average Accuracy = 0.9132\n",
      "Epoch: 43, Average Accuracy = 0.9135\n",
      "Epoch: 44, Average Accuracy = 0.9137\n",
      "Epoch: 45, Average Accuracy = 0.9136\n",
      "Epoch: 46, Average Accuracy = 0.9141\n",
      "Epoch: 47, Average Accuracy = 0.9144\n",
      "Epoch: 48, Average Accuracy = 0.9144\n",
      "Epoch: 49, Average Accuracy = 0.9146\n",
      "Epoch: 50, Average Accuracy = 0.9146\n",
      "Epoch: 51, Average Accuracy = 0.9147\n",
      "Epoch: 52, Average Accuracy = 0.9146\n",
      "Epoch: 53, Average Accuracy = 0.9145\n",
      "Epoch: 54, Average Accuracy = 0.9151\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters \n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "max_epoches = 200  # defines the maximum number of epoches (training will stop once meet the expected accuracy)\n",
    "\n",
    "# define the layers \n",
    "layers = [784, 10]\n",
    "\n",
    "# load the dataset \n",
    "x_train, x_test, y_train, y_test = load_dataset_mnist()\n",
    "\n",
    "# create an Neural Network Class \n",
    "nn = MyNeuralNetwork(layers, learning_rate, batch_size)\n",
    "nn.train_neural_network(x_train, y_train, x_test, y_test, max_epoches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- *Building a Neural Network from Scratch: Part 1*: https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/\n",
    "\n",
    "- *Neural Networks: Feedforward and Backpropagation Explained & Optimization*: https://mlfromscratch.com/neural-networks-explained/#/\n",
    "\n",
    "- *Neural networks and back-propagation explained in a simple way*: https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e\n",
    "\n",
    "- *深度学习－－手写数字识别*: https://blog.csdn.net/akadiao/article/details/78175737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
